**Q1: Difference between simple linear regression and multiple linear regression.**

* **Simple Linear Regression:** Involves one independent variable and one dependent variable.
  Example: Predicting house prices based on square footage.
  Equation: $y = b_0 + b_1x$
* **Multiple Linear Regression:** Involves two or more independent variables.
  Example: Predicting house prices based on square footage, number of bedrooms, and location.
  Equation: $y = b_0 + b_1x_1 + b_2x_2 + \ldots + b_nx_n$

---

**Q2: Assumptions of Linear Regression and Checking Them:**

1. **Linearity:** The relationship between the independent and dependent variables should be linear. Check using scatter plots and residual plots.
2. **Independence:** Observations should be independent. Check using Durbin-Watson test.
3. **Homoscedasticity:** Constant variance of residuals. Check using residual vs. fitted value plot.
4. **Normality of Residuals:** Residuals should be normally distributed. Check using Q-Q plots and the Shapiro-Wilk test.
5. **No Multicollinearity:** Independent variables should not be highly correlated. Check using VIF (Variance Inflation Factor).

---

**Q3: Interpreting Slope and Intercept in a Linear Regression Model:**

* **Slope (b1):** Indicates the change in the dependent variable for a one-unit increase in the independent variable.
  Example: If the slope is 0.5 in a salary prediction model, a one-unit increase in experience increases the salary by 0.5 units.
* **Intercept (b0):** The predicted value of the dependent variable when all independent variables are zero.

---

**Q4: Gradient Descent and Its Use in Machine Learning:**

* Gradient Descent is an optimization algorithm used to minimize the cost function by iteratively adjusting the model parameters.
* It calculates the gradient of the cost function and updates the parameters in the direction of the negative gradient to minimize the error.
* It is widely used in training machine learning models, especially in neural networks.

---

**Q5: Multiple Linear Regression Model and Its Differences from Simple Linear Regression:**

* Multiple Linear Regression includes more than one independent variable.
* Equation: $y = b_0 + b_1x_1 + b_2x_2 + \ldots + b_nx_n$
* Difference: Simple linear regression has only one predictor, whereas multiple linear regression has two or more predictors, allowing for more complex relationships.

---

**Q6: Multicollinearity in Multiple Linear Regression and How to Address It:**

* Multicollinearity occurs when two or more independent variables are highly correlated, causing instability in the coefficient estimates.
* Detection: Check VIF values; if VIF > 10, multicollinearity is a concern.
* Solutions:

  * Remove one of the correlated variables.
  * Combine correlated variables (e.g., using PCA).
  * Use Ridge or Lasso regression.

---

**Q7: Polynomial Regression and Its Differences from Linear Regression:**

* **Polynomial Regression:** A form of regression where the relationship between the independent and dependent variables is modeled as an $n^{th}$ degree polynomial.
  Equation: $y = b_0 + b_1x + b_2x^2 + \ldots + b_nx^n$
* Difference: Linear regression fits a straight line, while polynomial regression fits a curve, allowing for modeling nonlinear relationships.

---

**Q8: Advantages and Disadvantages of Polynomial Regression:**

* **Advantages:**

  * Captures complex, nonlinear relationships.
  * Provides a better fit for curved data.

* **Disadvantages:**

  * Prone to overfitting with high degrees.
  * Sensitive to outliers.
  * Computationally intensive.

* **When to Use:**

  * When the data shows a nonlinear trend that cannot be captured by a straight line, but can be approximated by a polynomial curve.
